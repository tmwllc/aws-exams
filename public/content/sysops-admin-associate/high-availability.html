<h1>High Availability Exam Tips</h1>
<h2>Elasticity</h2>
<p>Think of elasticity as a rubber band. It allows you to stretch out and retract back your infrastructure, based on demand. Only pay for what you need. Elasticity is used during a short time period, such as hours or days.</p>
<h2>Scalability</h2>
<p>Used to talk about building out the infrastructure to meet your long term needs. Scalability is used over longer time periods, such as weeks, days, months, and years.</p>

<h3>Scalability vs Elasticity in AWS Services</h3>
<ul>
    <li>EC2
        <ul>
            <li>Scalability - Increase instance sizes as require, using reserved instances</li>
            <li>Elasticity - Increase number of EC2 instances, based on autoscaling</li>
        </ul>
    </li>
    <li>
        <ul>
            <li>Scalability - </li>
            <li>Elasticity - </li>
        </ul>
    </li>
    <li>DynamoDB
        <ul>
            <li>Scalability - Unlimited storage</li>
            <li>Elasticity - Increase IOPS for additional traffic. Decrease afterward. DynamoDB now has autoscaling for this.</li>
        </ul>
    </li>
    <li>RDS
        <ul>
            <li>Scalability - Increase instance size</li>
            <li>Elasticity - Not very elastic, can't scale RDS based on demand</li>
        </ul>
    </li>
</ul>

<p>Exam may ask outdated question based on EC2 instances having better network throughput when you scale up. This is no longer the case, but exam may still ask about it.</p>
<p>Your infrastructure makes use a single t2Small NAT instance within a VPC to allow inside hosts to communicate with the Internet without being directly addressable. As your traffic has grown, the amount of traffic going through the NAT has increased and is overwhelming it which is slowing down your infrastructure. What 2 options should you do to alleviate this issue?</p>
<ul>
  <li>A - Add another internet gateway to your VPC</li>
  <li>B - Increase the instance size of the NAT from t2.Small to t2.medium</li>
  <li>C - Use Direct Copnnect to route all traffic instead</li>
  <li>D 0 Add another NAT instance and configure your subnet route tables to be spread across two NATs</li>
</ul>
<p>Answer is B and D.</p>

<h3>Exam Tips</h3>
<ul>
  <li>If the bottleneck is network related, the answer is to scale up by upgrading instance class.</li>
  <li>If problem is in relation to not having enough resources (can't increase size further) the answer is to scale out (add more instances).</li>
  <li>Remember elasticity. Scaling out, you can scale back. Scaling up is easy, scaling down is not so easy.</li>
</ul>

<h3>RDS</h3>
<h4>Multi-AZ</h4>
<p>DB instances use a DNS name, called an RDS endpoint. AWS will failover that name from your primary to your secondary in the event of a DB failure on the primary. The DNS name does not change, but the IP it points to does.</p>
<h4>Multi-AZ Failover Advantages</h4>
<ul>
  <li>High Availability</li>
  <li>Backsups are taken from secondary which avoids IO suspension to the primary</li>
  <li>Restores are taken from the secondary, which avoids IO suspension to the primary</li>
  <li>You can force a failover by rebooting the primary instance in the console, or by using the RebootDbInstance API call.</li>
</ul>
<p>RDS is for DR, not for scaling. Read Replicas are used for scaling, not for DR.</p>

<h3>Read Replicas</h3>
<p>Read Replicas are a read only copy of your DB. Allows you to scale out to alleviate read constraints. Can create a Read Replica with a few clicks in the console or using the CreateDBInstanceReadReplica API. Once created, DB updates on the source DB instance will be replicated using a supported engine's native, ascynhronous replication. You can create multiple Read Replicas for a given source DB instance and distribute your application's read traffic amongst them.</p>
<h4>When to Use</h4>
<ul>
  <li>Scaling beyond the compute or IO capacity of a single DB instance for read heavy DB workloads. This excess read traffic can be directed to one or more Read Replicas.</li>
  <li>Serving read traffic while source DB is unavailable, due to suspension or scheduled maintenance, you can direct read traffic to your Read Replicas.</li>
  <li>Business reporting or data warehousing scenarios; you may want business reporting queries to run against a Read Replicas, rather than your primary production DB.</li>
</ul>
<h4>Supported Versions</h4>
<ul>
  <li>MySQL
    <ul>
      <li>v5.6 (not 5.1 or 5.5)</li>
      <li>Can use both MySQL engines (MyISAM and InnoDB), however only InnoDB is supported by AWS</li>
    </ul>
  </li>
  <li>PostGreSQL
    <ul>
      <li>v.9.3.5+</li>
    </ul>
  </li>
  <li>Aurora</li>
  <li>MariaDB</li>
</ul>
<h4>Creating Read Replicas</h4>
<ul>
  <li>AWS will take a snapshot of your DB</li>
  <li>If Multi-AZ is not enabled, snapshot will be of primary DB and can cause brief IO suspension for around 1 minute</li>
  <li>If Mutli-AZ is enabled, snapshot will be secondary DB and will not experience any performance hits on primary DB.</li>
</ul>
<p>When connecting to a new Read Replica you will use a new endpoint DNS address. Will have to update application to use that.</p>
<p>Read Replicas can be promoted to its own standalone DB. Doing this will break replication.</p>
<ul>
  <li>Can have up to 5 Read Replicas in MySQL and PostGreSQL.</li>
  <li>Can have Read Replicas in different regions, but for MySQL only.</li>
  <li>Replication is asynchronous only</li>
  <li>Read Replicas can be built off Multi AZ DBs</li>
  <li>Read Replicas CANNOT be Multi-AZ currently</li>
  <li>Can have Read Replicas of Read Replicas, but only for MySQL, and this will further increase latency.</li>
  <li>DB snapshots and automated backups CANNOT be taken off Read Replicas</li>
  <li>Key metric to look for is ReplicaLag.</li>
  <li>Read Replicas require backups to be enabled</li>
</ul>

<p>Can scale up, switch to Multi-AZ, enable backups without altering the DNS name of the instance. However, restoring from a backup/snapshot will always create a new DB with new DNS name. Scaling up will result in some downtime, though, so should be done during off hours, or restore from a backup and switch the app to point to the new DNS name. </p>
<p>Cannot have multiple AZs enabled on Read Replicas. You can have multiple Read Replicas in different AZs, though.</p>
<p>If you want to force a failover from primary DB to secondary DB in different AZ, reboot the primary. You'll see option for "reboot with failover". Select that and click ok.</p>

<h3>Bastion Hosts</h3>
<p>For high availability, setup 2+ bastion hosts in different AZs/Subnets. Use Route53 Failover routing to send traffic to primary/secondary Bastion host.</p>

<h3>Troubleshooting Autoscaling</h3>
<p>Things to look for if instances are not launching into ASG</p>
<ul>
    <li>Associated Key Pair does not exist</li>
    <li>SG does not exist</li>
    <li>Autoscaling config is not working correctly</li>
    <li>Autoscaling group not found</li>
    <li>Instance type specified is not supported in the AZ</li>
    <li>AZ is no longer supported</li>
    <li>Invalid EBS device mapping</li>
    <li>Autoscaling service is not enabled on account</li>
    <li>Attempting to attach an EBS block device to an instance-store AMI</li>
    <li></li>
</ul>

<h3>Launch Configuration</h3>
<ul>
  <li>Specifies what type of instance to start, just like when creating a new EC2 instance</li>
  <li>Each ASG must have a launch configuration</li>
  <li>Cannot modify launch configuration after creation</li>
  <li>If ASG needs to alter the launch configuration, you must create a new one</li>
  <li>When you change the launch configuration for an existing ASG, new instances launched will use the new launch configuration, but existing instances are unaffected</li>
</ul>

<h3>Auto Scaling Group</h3>
<ul>
  <li>Cannot span regions, but can span AZs</li>
  <li>Auto Scaling automatically balances instances between AZs</li>
  <li>Will not launch an instance in an AZ if there are no instances available in that AZ</li>
  <li>Cannot add a volume when it's getting full. This must be done manually via the EC2 instance itself.</li>
</ul>

<h3>Lifecycle Hooks</h3>
<ul>
  <li>Allow you to run software on a newly launched instance prior to auto scaling connects it to ELB</li>
  <li>Using terminate hooks, you can save data on the drive before it's deleted, such as sending log files to S3</li>
</ul>

<h3>Health Checks</h3>
<ul>
  <li>Auto scaling performs health checks on EC2 instances at regular intervals</li>
  <li>If using ELB, should select ELB health check</li>
  <li>Can force a health check to fail by using SetInstanceHealth API to set instance's state to UNHEALTHY, which results in termination and replacement</li>
  <li>Health checks can be temporarily suspended using SuspendProcesses API. Use ResumeProcesses API to resume health checks</li>
</ul>

<h3>Stateful Instances</h3>
<p>These are instances with data on them. If Auto Scaling terminates them, that data is lost. So it's a good idea to use LifeCycle Hooks to save the data before termination, or use EC2 instance protection to prevent Auto Scaling from terminating it.</p>

<h3>Impaired Instances</h3>
<p>When Auto Scaling detects that an instance is impaired, it automatically terminates it and replaces it. If using an ELB, it detaches it from ELB first before terminating it. When scaling in, you can use a termination policy to specify which instance(s) is terminated. Can also set termination protection on some instances to avoid having Auto Scaling terminate them. When ELB notices that an instance is unhealthy, it will stop routing traffic to it.</p>

<h3>Application Auto Scaling</h3>
<ul>
  <li>Define scaling policies to automatically scale AWS resources</li>
  <li>Scale resources in response to Cl;oudWatch Alarms</li>
  <li>View the history of scaling events</li>
</ul>
<h4>Can scale the following resources:</h4>
<ul>
  <li>ECS Services</li>
  <li>EC2 Spot fleets</li>
  <li>EMR clusters</li>
  <li>AppStream 2.0 fleets</li>
  <li>Provisioned read/write capacity for DynamoDB tables and global secondary indexes</li>
</ul>
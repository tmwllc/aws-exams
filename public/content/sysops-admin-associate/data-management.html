<h1>Data Management</h1>
<h3>Disaster Recovery</h3>
<p><a href="http://media.amazonwebservices.com/AWS_Disaster_Recovery.pdf" target="_blank">AWS Disaster Recovery Whitepaper</a></p>

<h4>Services that support Disaster Recovery</h4>
<ul>
  <li>Storage Gateway
    <ul>
      <li>Gateway-cached volumes - store primary data and cache most recently used data locally</li>
      <li>Gateway-stored volumes - store entire dataset on site and asynchronously replicate back to S3</li>
      <li>Gateway-virtual tape library - Store virtual tapes in S3 or Glacier</li>
    </ul>
  </li>
</ul>

<h3>RTO vs RPO</h3>
<p>Recovery Time Objective (RTO) is the length of time from which you can recover from a disaster. It is measured from when the disaster first occurred to when you have fully recovered from it.</p>
<p>Recovery Point Objective (RPO) is the amount of data your organization is prepared to lose in the event of a disaster (1 days worth of emails, 5 hours of online transaction records, 24 hours of backup, etc.)</p>
<p>Typically the lower the RTO and RPO threshold, the more costly the solution will be.</p>

<ul>
  <li>Backup & Restore
    <ul>
      <li>Data is backed up to tape and sent off site regularly.</li>
      <li>Can take a long time to restore system.</li>
      <li>S3 is ideal destination for backup data.</li>
      <li>Transferring data to/from S3 is typically done through the network and is therefore accessible from any location.</li>
      <li>Can use AWS Import/Export (or Snowball) to transfer very large data sets by shipping storage devices directly to AWS.</li>
      <li>For longer-term data storage where retrieval times of several hours are adequate, Glacier is ideal. It has the same durability as S3. Glacier and S3 can be used in conjunction to produce a tiered backup solution.
      </li>
      <li>Key Steps for Backup/Restore
        <ul>
          <li>Select appropriate tool/method to backup data into AWS</li>
          <li>Ensure that you have an appropriate retention policy for this data</li>
          <li>Ensure security measures are in place for this data, including encryption and access policies</li>
          <li>Regularly test the recovery of this data and restoration of your system</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Pilot Light -
    <ul>
      <li>Minimal version of an environment is always running in the cloud</li>
      <li>Scenario is similar to Backup/Restore.</li>
      <li>Maintain a pilot light by configuring/running the most critical core elements of your system in AWS</li>
      <li>Can rapidly provision a full-scale production environment around the critical core</li>
      <li>Infrastrucre elements for pilot light typically include DB servers which would replicate data to EC2 or RDS</li>
      <li>Might be other data outside of DB that needs to be replicated to AWS.</li>
      <li>This is the critical core of the system (pilot light) around which all other infrastructure pieces in AWS can quickly be provisioned to restore the complete system.</li>
      <li>To provision the remainder of the the infrastructure, you would have some preconfigured servers bundled as AMIS which are ready to be started up at a moment's notice.</li>
      <li>When starting recovery, instances from those AMIs come up quickly with their pre-defined role (web or app server)</li>
      <li>Two Options:
        <ul>
          <li>Use pre-allocated elastic IP addresses and associate them with your instances when invoking DR. you can also use pre-allocated elastic network interfaces (ENIs) with pre-allocated MAC addresses for apps with special licensing requirements. (THIS COMES UP ON EXAM A LOT)</li>
          <li>Use ELB to distribute traffic to multiple instances. Then update DNS to point at your EC2 instance or point to your load balancer using a CNAME.</li>
        </ul>
      </li>
      <li></li>
    </ul>
  </li>
  <li>Warm Standby
    <ul>
      <li>DR scenario in which a scaled-down version of a fully functional environment is always running in the cloud.</li>
      <li>Extends pilot light elements and preparation.</li>
      <li>Decreases RTO (over Pilot Light) because some services are always running.</li>
      <li>By identifying business-critical systems, can fully duplicate these on AWS and have them always on.</li>
      <li>Servers can be running on the minimum-sized fleet of EC2 instances on smallest sizes possible.</li>
      <li>Solution is not scaled to take full production workload, but is fully functional.</li>
      <li>Can be used for non-production work, such as testing, quality assurance and internal use.</li>
      <li>System is scaled up quickly to handle the production load.</li>
      <li>Can be done by adding more instances to the load balancer and by resizing the small capacity servers to run on larger EC2 instance types.</li>
      <li>Horizontal scaling is preferred over vertical scaling.</li>
      <li>Key Steps for Setup:
        <ul>
          <li>Set up EC2 instances to replicate data</li>
          <li>Create and maintain AMIs</li>
          <li>Run application using minimal footprint of EC2 instances</li>
          <li>Patch and update software and configuration files in line with your live environment</li>
        </ul>
      </li>
      <li>Key Steps for Recovery:
        <ul>
          <li>Increase size of EC2 fleets with load balancer (horizontal scaling)</li>
          <li>Start apps on larger EC2 instances (vertical scaling)</li>
          <li>Manually change DNS records, or use Route53 automated health checks so that all traffic is routed to AWS environment</li>
          <li>Consider using auto scaling to right-size the fleet or accommodate the increased load.</li>
          <li>Add resilience  or scale up DB.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Multi Site
    <ul>
      <li>Runs in AWS as well as your existing on-site infrastructure</li>
      <li>Data replication method determined by recovery point that you choose</li>
      <li>Can use Route53 to route traffic to both sites symmetrically or asymmetrically</li>
      <li>In an on-site disaster, can adjust DNS weighting to send all traffic to AWS servers</li>
      <li>Capacity of AWS service can be rapidly increased to handle production load.</li>
      <li>Can use EC2 auto scaling to automate process.</li>
      <li>Might need some app logic to detect failure of primary DB and cut over to parallel DB in AWS.</li>
      <li>Key Steps:
        <ul>
          <li>Setup AWS environment to duplicate production env</li>
          <li>Setup DNS weighting or similar traffic routing technology to distribute incoming requests to both sites</li>
          <li>Configure automated failover to re-route traffic away from affected site</li>
        </ul>
      </li>
      <li>Preparation:
        <ul>
          <li>Manually or by using DNS failover, change DNS weighting so that all requests are sent to AWS site.</li>
          <li>Have app logic for failover to use the local AWS DB for all queries</li>
          <li>Consider using auto scaling to right-size the fleet or accommodate the increased load.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Failing Back:
    <ul>
      <li>Backup/Restore:
        <ul>
          <li>Freeze data changes to the DR site</li>
          <li>Take a backup</li>
          <li>Restore back to primary site</li>
          <li>Re-point users to the primary site</li>
          <li>Unfreeze changes</li>
        </ul>
      </li>
      <li>Pilot Light, Warm Standy, Multi-Site:
        <ul>
          <li>Establish reverse mirroring/replication from DR back to primary</li>
          <li>Freeze data changes to DR site</li>
          <li>Re-point users to primary site</li>
          <li>Unfreeze changes</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p>Pay particular attention to Pilot Light scenario, and the fact that you can have ENIs with preconfigured MAC addresses</p>

<h3>Services with Automated Backups</h3>
<ul>
  <li>RDS
    <ul>
      <li>For MySQL need InnoDB (transactional engine)</li>
      <li>For the other RDS types (SQL Server, PostGreSQL, Oracle, Aurora, MariaDB) support automated backups</li>
      <li>Performance hit if Multi-AZ is not enabled</li>
      <li>If you delete an instance, all automated backups are deleted</li>
      <li>Manual backups are not deleted on instance delete</li>
      <li>All backups stored on S3</li>
      <li>When you restore, you can change engine type (SQL Standard to SQL Enterprise for example), provided you have enough storage space.</li>
    </ul>
  </li>
  <li>Elasticache (Redis only, not Memcached)
    <ul>
      <li>Entire cluster is snapshotted</li>
      <li>Snapshot will degrade performance</li>
      <li>Therefore only set your snapshot window during the least busy part of the day</li>
      <li>Stored on S3</li>
    </ul>
  </li>
  <li>Redshift
    <ul>
      <li>Stored on S3</li>
      <li>By default, Redshift enables automated backups of your cluster with a 1-day retention period</li>
      <li>Redshift only backs up data that has changed so most snapshots only use up a small amount of your free backup storage</li>
    </ul>
  </li>
</ul>
<h3>Services without Automated Backups</h3>
<ul>
  <li>EC2
    <ul>
      <li>No automated backups</li>
      <li>Can do manual snapshots, but they will degrade performance. Schedule these during non-busy time.</li>
      <li>Snapshots are stored in S3</li>
      <li>Can create automated backups using either the CLI or Python</li>
      <li>They are incremental:
        <ul>
          <li>Snapshots only store incremental changes since last snapshot</li>
          <li>Only charged for incremental storage</li>
          <li>Each snapshot still contains the base snapshot data</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3>EC2 Instance Types</h3>
<h4>Volume Types</h4>
<ul>
  <li>Root volume
    <ul>
      <li>Can be either EBS or Instance Store</li>
      <li>Instance store root max size is 10GB</li>
      <li>EBS root can be up to 1 or 2TB depending on the OS</li>
      <li>When terminating instance:
        <ul>
          <li>EBS root is terminated by default. This can be disabled by unchecking "Delete on Termination" or by setting the deleteontermination flag to false via the CLI.</li>
          <li>Non-root EBS volumes are preserved when EC2 instance is deleted</li>
          <li>Instance store root volumes are terminated by default when EC2 instance is terminated. Cannot disable this.</li>
          <li>Other instance store volumes will also be terminated on EC2 termination.</li>
          <li>Other EBS volumes attached to the instance will persist automatically.</li>
          <li>EBS volumes can changed on the fly (except for magnetic standard, which must be stopped, detached, snapshotted and create new volume from it)</li>
          <li>Best practice to stop/detach/snapshot volume when upgrading it</li>
          <li>If you change volume on the fly, must wait 6 hours before making another change</li>
          <li>Can scale EBS volumes up only, not down</li>
          <li>Volumes must be in the same AZ as EC2 instance.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Additional volume</li>
</ul>
<p>Data in an instance store persists only during t he lifetime of its associated instance. If instance reboots, data in the instance store perists. However, data on instance store volumes is lost under the following circumstances:</p>
<ul>
  <li>Failure of an underlying drive</li>
  <li>Stopping an EBS-backed instance</li>
  <li>Terminating an instance</li>
</ul>
<p>To keep data safe, don't use instance store volumes. Use a replication strategy across multiple instances, storage data in S3 or use EBS volumes. Additional EBS volumes persist when EC2 is terminated. So need to clean those up manually.</p>

<h4>To Upgrade a non-root EBS Volume</h4>
<ul>
  <li>Unmount volume while SSHed into instance</li>
  <li>Detach volume in EC2 console</li>
  <li>Create snapshot of volume</li>
  <li>Select snapshot and then Actions -> Create New Volume, using desired volume specs</li>
  <li>Attach new volume to EC2 instance</li>
</ul>

<h4>Exam Scenario</h4>
<p>You need to implement tiered storage for your DB backups and logs. The backups need to be archived to a durable storage solution at EOD. After 28 days, the backups must be archived off to cheaper storage but must still be retained. Which tiered storage proposal meets the recovery scenario, minimizes costs and addresses the requirements?</p>
<ul>
  <li>A - Use an independent EBS volume to store backups and log files. Take daily snapshots of this volume. After 28 days rotate your EBS snapshots.</li>
  <li>B - At EOD back up DB and copy backup files to S3. After 28 days copy the data from S3 to RDS Log File Collector.</li>
  <li>C - Store log files on EC2 ephemeral storage and then randomly terminate your EC2 instances.</li>
  <li><b>D - Use an independent EBS volume to store daily backups and copy the backup files to S3. Create a lifecycle policy on S3 to archive off the backups to Glacier after 28 days.</b></li>
</ul>